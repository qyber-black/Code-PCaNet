#!/usr/bin/env python3
#
# pca.py - PCa classification and segmentation
#
# SPDX-FileCopyrightText: Copyright (C) 2021-2023 Frank C Langbein <frank@langbein.org>, Cardiff University
# SPDX-License-Identifier: AGPL-3.0-or-later
#
# See --help for arguments, uses sub-commands

import os
import sys
import argparse

from pcanet.cfg import Cfg
from pcanet.pcnn import get_pcnn_names

def main():
  # Main function to parse arguments and then call sub-command

  # Process arguments
  parser = argparse.ArgumentParser(description='Prostate cancer classification and segmentation',
                                   formatter_class=argparse.ArgumentDefaultsHelpFormatter)
  subparsers = parser.add_subparsers(title="Valid sub-commands")

  # Augmentation
  p_aug = subparsers.add_parser('augment', help='Augment dataset',
                                formatter_class=argparse.ArgumentDefaultsHelpFormatter)
  p_aug.add_argument('-d', '--data', help='Folder containing data for augmentation')
  p_aug.add_argument('-c', '--channels', type=str, nargs='+',
                     help='List of channels to process')
  p_aug.add_argument('-t', '--tags', type=str, nargs='+', default=["normal", "suspicious"],
                     help='List of channels that are tags for classification; expected two; include these in the channels, too, or they will not be found; if this is "piradsX1:X2:..." mask with >=Xn & <Xn+1, etc. on pirads channel')
  p_aug.add_argument('--shape', type=int, nargs=2, default=None,
                     help='Shape WIDTH HEIGHT of output slices; if both are 0, then use shape of first stack (sorted alphabetically); if not specified (None), do not resize; we assume each stack is aligned and has the same shape, but shape may not be consistent across stacks')
  p_aug.add_argument('-f', '--factor', type=int, default=40,
                     help='Augmentation factor')
  p_aug.add_argument('-s', '--square', action='store_true',
                     help='Crop to bounding square; for classify mode only')
  p_aug.add_argument('-m', '--mode', type=str, default="classify",
                     help='Augmentation mode: classify, segment')
  p_aug.add_argument('-o', '--output', help='Folder name to store augmented dataset; should not contain _')
  p_aug.add_argument('-p', '--disable-parallel', action='store_true', help='Do not execute in parallel (for testing, etc)')
  p_aug.add_argument('-v', '--verbose', action='count', help='Increase output verbosity (0: none; 1: main text; 2: +main plots; 3: detailed text; 4: +detailed plots; 5: +tests and debug; 6: +extra debug/test plots)', default=0)
  p_aug.set_defaults(func=augment)

  # Classifier
  p_cla = subparsers.add_parser('classify', help='Classify: train if model does not exist otherwise evaluate on dataset',
                                formatter_class=argparse.ArgumentDefaultsHelpFormatter)
  p_cla.add_argument('-m', '--model', type=str,
                     help='Model folder - train only if model does not exist; this is the folder where the results and models are stored/loaded from')
  p_cla.add_argument('-f', '--fold', type=int, default=0,
                     help='Select fold for model; depends on archtecture if there is a model per fold; evaluate only')
  p_cla.add_argument('-d', '--data', type=str,
                     help='Dataset folder name, generated by augment; tags are sorted alphabetically and second tag is assumed positive for evaluation')
  p_cla.add_argument('-c', '--channels', type=str, nargs='+', default=['t2-tra', 'adc'],
                     help='List of channels to use; cannot be empty for trainig; training only')
  p_cla.add_argument('-s', '--size', type=int,
                     help='Size of square input to scale data for training; sizes available depend on architecture; 0 - do not resize; training only',
                     default=16)
  pcnn_models = "|".join(get_pcnn_names())
  p_cla.add_argument('-a', '--architecture', default='svmsbfs:poly:1:first_haralick_lbp:std',
                     help='Classifier architecture: svm[sbfs]:{linear,poly,rbf,sigmoid}:C:{first_haralick_lbp_CNN_...}[:std][:norm]; rfc[sbfs]:ESTIMATORS:MAXDEPTH:MAX_FEATURES:MIN_SAMPLES_SPLIT:MIN_SAMPLES_LEAF:{first_haralick_lbp_CNN_...}[:std][:norm]; pcnn:CNN:FC:DO:{conc,avg,add,wconc,wavg,wadd}:FT[_EPOCHS]:BATCH_SIZE:EPOCHS:{bc[l[c]],bfc[l[c]],hinge,kld}:[:rgb_train]; CNN='+pcnn_models+';training only')
  p_cla.add_argument('--balance', action='store_true',
                     help='Balance augmented dataset by adjusting augmentation factor per class to have approx. same number of samples')
  p_cla.add_argument('-p', '--disable-parallel', action='store_true', help='Do not execute in parallel (disable for testing, etc)')
  p_cla.add_argument('-v', '--verbose', action='count', help='Increase output verbosity (0: none; 1: main text; 2: +main plots; 3: detailed text; 4: +detailed plots; 5: +tests and debug; 6: +extra debug/test plots)', default=0)
  p_cla.set_defaults(func=classify)

  # Predict
  p_pre = subparsers.add_parser('predict', help='Get model prediction; no evaluation/known outputs',
                                formatter_class=argparse.ArgumentDefaultsHelpFormatter)
  p_pre.add_argument('-m', '--model', type=str,
                     help='Model file - folder containing the model')
  p_pre.add_argument('-f', '--fold', type=int, default=0,
                     help='Select fold for model; depends on architecture if there is a model per fold')
  p_pre.add_argument('-d', '--data', type=str,
                     help='Dataset folder name, generated by augment')
  p_pre.add_argument('-p', '--disable-parallel', action='store_true', help='Do not execute in parallel (disable for testing, etc)')
  p_pre.add_argument('-v', '--verbose', action='count', help='Increase output verbosity (0: none; 1: main text; 2: +main plots; 3: detailed text; 4: +detailed plots; 5: +tests and debug; 6: +extra debug/test plots)', default=0)
  p_pre.set_defaults(func=predict)

  # View augmented dataset
  p_view = subparsers.add_parser('view',
                                 help='View augmented dataset',
                                 formatter_class=argparse.ArgumentDefaultsHelpFormatter)
  p_view.add_argument('-d', '--data', type=str,
                      help='Dataset file as generated by augment to view')
  p_view.add_argument('-v', '--verbose', action='count', help='Increase output verbosity (0: none; 1: main text; 2: +main plots; 3: detailed text; 4: +detailed plots; 5: +tests and debug; 6: +extra debug/test plots)', default=0)
  p_view.set_defaults(func=view)

  # Parse
  args = parser.parse_args()
  if hasattr(args,"func"):
    args.func(args)
  else:
    print("%s: illegal sub-command or sub-command not specified, see help [-h]" % sys.argv[0], file=sys.stderr)

def augment(args):
  # Dataset augmentation
  import pcanet.dataset as dataset
  from pcanet.augment import Augment
  if args.data is None:
    raise Exception("No dataset path")

  # Setup dataset and augmentation
  ds = dataset.Dataset(args.data, args.channels, args.shape, verbose=args.verbose)
  # Find masks and start augmentation - may need to change list if wider range of channels is used
  masks = [l for l,c in enumerate(args.channels) if c[0:3] != 'adc' and
                                                    c[0:3] != 'dwi' and
                                                    c[0:2] != 't2'  and
                                                    c[0:2] != 't1']
  aug = Augment(masks=masks, factor=args.factor, parallel=(not args.disable_parallel), verbose=args.verbose)
  if args.mode == "classify":
    aug.add_default_classify()
    # Folder for dataset, name important as it is used to recover channel and factor information
    if len(args.tags) == 1 and args.tags[0][0:6] == "pirads":
      from pcanet.crop import crop_classify_pirads
      if args.tags[0] == "pirads":
        pirads_thresholds = [1,2,3,4,5]
      else:
        pirads_thresholds = [int(t) for t in args.tags[0][6:].split(":")]
      chs = args.channels.copy()
      chs[chs.index("pirads")] = "pirads" + ":".join([str(t) for t in pirads_thresholds])
      path = args.output+"_"+str(args.factor)+"_"+"_".join(chs)+"_classify"
      post_process = lambda slices, pid : dataset.ClassifyDataset.save_classify(
                                              crop_classify_pirads(slices, pirads_thresholds, args.channels, args.square,
                                                                   verbose=args.verbose if args.disable_parallel else 0),
                                            pid, path)
    else:
      path = args.output+"_"+str(args.factor)+"_"+"_".join(args.channels)+"_classify"
      from pcanet.crop import crop_classify
      crop_masks = [l for l,c in enumerate(args.channels) if c in args.tags]
      post_process = lambda slices, pid : dataset.ClassifyDataset.save_classify(
                                              crop_classify(slices, crop_masks, args.channels, args.square,
                                                            verbose=args.verbose if args.disable_parallel else 0),
                                            pid, path)
  elif args.mode == "segment":
    aug.add_default_segment()
    # Folder for dataset, name important as it is used to recover channel and factor information
    if len(args.tags) == 1 and args.tags[0][0:6] == "pirads":
      from pcanet.crop import segment_pirads
      if args.tags[0] == "pirads":
        pirads_thresholds = [0,1,2,3,4,5]
      else:
        pirads_thresholds = [int(t) for t in args.tags[0][6:].split(":")]
      chs = args.channels.copy()
      chs[chs.index("pirads")] = "pirads" + ":".join([str(t) for t in pirads_thresholds])
      path = args.output+"_"+str(args.factor)+"_"+"_".join(chs)+"_segment"
      post_process = lambda slices, pid : dataset.SegmentationDataset.save_segment(
                                              segment_pirads(slices, pirads_thresholds, args.channels,
                                                             verbose=args.verbose if args.disable_parallel else 0),
                                            pid, path)
    else:
      path = args.output+"_"+str(args.factor)+"_"+"_".join(args.channels)+"_segment"
      post_process = lambda slices, pid : dataset.SegmentationDataset.save_segment(slices, pid, path, args.verbose)
  else:
    raise Exception(f"Unknown augmentation mode {args.mode}")

  # Apply out-of-core augmentation and process/store results via post_process
  aug.apply(ds, post_process)

def classify(args):
  from pcanet.dataset import ClassifyDataset
  from pcanet.classify import Classify

  # FIXME: SHAP, LIME?

  data_ext = None
  classifier = None
  if os.path.exists(args.model):
    # Classify, if model already exists
    train = False
    if os.path.exists(os.path.join(args.model,"svmsbfs_classifier.joblib")):
      from pcanet.classify_svmsbfs import SVMSBFSClassifier
      classifier = SVMSBFSClassifier.load(args.model, args.verbose)
    elif os.path.exists(os.path.join(args.model,"svm_classifier.joblib")):
      from pcanet.classify_svm import SVMClassifier
      classifier = SVMClassifier.load(args.model, args.verbose)
    elif os.path.exists(os.path.join(args.model,"rfcsbfs_classifier.joblib")):
      from pcanet.classify_rfcsbfs import RFCSBFSClassifier
      classifier = RFCSBFSClassifier.load(args.model, args.verbose)
    elif os.path.exists(os.path.join(args.model,"rfc_classifier.joblib")):
      from pcanet.classify_rfc import RFCClassifier
      classifier = RFCClassifier.load(args.model, args.verbose)
    elif os.path.exists(os.path.join(args.model,f"pcnn_tf_{args.fold}")):
      from pcanet.classify_pcnn import PCNNClassifier
      classifier = PCNNClassifier.load(args.model, args.fold, args.verbose)
      data_ext = f"fold{args.fold}"
  if classifier is None:
    # Train: setup classifier
    train = True
    arch = args.architecture.split(":")
    if arch[0] == 'svm' or arch[0] == 'svmsbfs':
      # svm[sbfs]:{linear,poly,rbf,sigmoid}:C:{first_haralick_lbp_CNN_...}[:norm][:std]
      norm = False
      std = False
      for l in range(4,len(arch)):
        if arch[l] == "std":
          std = True
        elif arch[l] == "norm":
          norm = True
        else:
          raise Exception(f"Unknown architecture argument {arch[l]}")
      if arch[0] == 'svm':
        from pcanet.classify_svm import SVMClassifier
        classifier = SVMClassifier(model=args.model, kernel=arch[1], C=float(arch[2]), inp_features=arch[3],
                                   size=args.size, channels=args.channels, standardise=std,
                                   normalise=norm, verbose=args.verbose)
      else:
        from pcanet.classify_svmsbfs import SVMSBFSClassifier
        classifier = SVMSBFSClassifier(model=args.model, kernel=arch[1], C=float(arch[2]), inp_features=arch[3],
                                       size=args.size, channels=args.channels, standardise=std,
                                       normalise=norm, verbose=args.verbose)
    elif arch[0] == 'rfc' or arch[0] == 'rfcsbfs':
      # rfc[sbfs]:ESTIMATORS:MAXDEPTH:MAX_FEATURES:MIN_SAMPLES_SPLIT:MIN_SAMPLES_LEAF:{first_haralick_lbp_CNN_...}[:norm][:std]
      norm = False
      std = False
      for l in range(7,len(arch)):
        if arch[l] == "std":
          std = True
        elif arch[l] == "norm":
          norm = True
        else:
          raise Exception(f"Unknown architecture argument {arch[l]}")
      if arch[0] == 'rfc':
        from pcanet.classify_rfc import RFCClassifier
        classifier = RFCClassifier(model=args.model, n_estimators=int(arch[1]), max_depth=int(arch[2]),
                                   max_features=arch[3], min_samples_split=arch[4], min_samples_leaf=arch[5],
                                   inp_features=arch[6],
                                   size=args.size, channels=args.channels, standardise=std,
                                   normalise=norm, parallel=(not args.disable_parallel), verbose=args.verbose)
      else:
        from pcanet.classify_rfcsbfs import RFCSBFSClassifier
        classifier = RFCSBFSClassifier(model=args.model, n_estimators=int(arch[1]), max_depth=int(arch[2]),
                                       max_features=arch[3], min_samples_split=arch[4], min_samples_leaf=arch[5],
                                       inp_features=arch[6],
                                       size=args.size, channels=args.channels, standardise=std,
                                       normalise=norm, parallel=(not args.disable_parallel), verbose=args.verbose)
    elif arch[0] == 'pcnn':
      # pcnn:{VGG16,InceptionV3,ResNet50,MobileNetV2,CNN}:FC:DO:{conc,avg,wconc,wavg}:FT[_EPOCHS]:BATCH_SIZE:EPOCHS:{bc[l],bfc[l],hinge,kld}[:rgb_train]
      from pcanet.classify_pcnn import PCNNClassifier
      rgbt = False
      for a in range(9,len(arch)):
        if arch[a] == "rgb_train":
          rgbt = True
        else:
          raise Exception(f"Unknown flag {arch[a]}")
      if arch[5] != "0":
        s = arch[5].split("_")
        ft_layers = int(s[0])
        if len(s) < 2:
          ft_epochs = int(arch[7])
        else:
          ft_epochs = int(s[1])
      else:
        ft_layers = 0
        ft_epochs = 0
      classifier = PCNNClassifier(model=args.model, type=arch[1], fc=int(arch[2]), do=float(arch[3]),
                                  rgb_train=rgbt, merge=arch[4], ft_layers=ft_layers,
                                  batch_size=int(arch[6]), epochs=int(arch[7]), loss=arch[8], ft_epochs=ft_epochs,
                                  size=args.size, channels=args.channels, verbose=args.verbose)
    else:
      raise Exception(f"Unknown classifier architecture {args.architecture}")

  # Prepare dataset
  args.data = os.path.normpath(args.data)
  typename = os.path.basename(args.data).split("_")[-1]
  if typename != "classify":
    raise Exception("Not a classification dataset")
  ch = None
  # Get channels from classifier, not args (in case it has been loaded)
  # Info can be in estimator or directly in pca of the classifier object
  if hasattr(classifier, 'estimator') and hasattr(classifier.estimator, 'pca'):
    ch = classifier.estimator.pca['channels']
  elif hasattr(classifier, 'pca'):
    ch = classifier.pca['channels']
  ds = ClassifyDataset(args.data, channels=ch, verbose=args.verbose)

  # Setup classifier and train, if not loaded
  trainer = Classify(classifier, parallel=(not args.disable_parallel), verbose=args.verbose)
  if train:
    trainer.preprocess(ds, 5, args.balance)
    trainer.fit()
  else:
    trainer.preprocess(ds, 1)

  # Evaluate
  trainer.evaluate(data_ext=data_ext)

def predict(args):
  from pcanet.dataset import ClassifyDataset
  from pcanet.classify import Classify

  # Load model
  if not os.path.exists(args.model):
    raise Exception(f"Model {args.model} not found")

  # Classify, if model already exists
  data_ext = None
  if os.path.exists(os.path.join(args.model,"svmsbfs_classifier.joblib")):
    from pcanet.classify_svmsbfs import SVMSBFSClassifier
    classifier = SVMSBFSClassifier.load(args.model, args.verbose)
  elif os.path.exists(os.path.join(args.model,"svm_classifier.joblib")):
    from pcanet.classify_svm import SVMClassifier
    classifier = SVMClassifier.load(args.model, args.verbose)
  elif os.path.exists(os.path.join(args.model,"rfcsbfs_classifier.joblib")):
    from pcanet.classify_rfcsbfs import RFCSBFSClassifier
    classifier = RFCSBFSClassifier.load(args.model, args.verbose)
  elif os.path.exists(os.path.join(args.model,"rfc_classifier.joblib")):
    from pcanet.classify_rfc import RFCClassifier
    classifier = RFCClassifier.load(args.model, args.verbose)
  elif os.path.exists(os.path.join(args.model,f"pcnn_tf_{args.fold}")):
    from pcanet.classify_pcnn import PCNNClassifier
    classifier = PCNNClassifier.load(args.model, args.fold, args.verbose)
    data_ext = f"fold{args.fold}"
  else:
    raise Exception(f"Unknown model {args.model}")

  # Prepare dataset
  args.data = os.path.normpath(args.data)
  typename = os.path.basename(args.data).split("_")[-1]
  if typename != "classify":
    raise Exception("Not a classification dataset")
  # Get channels from classifier
  # Info can be in estimator or directly in pca of the classifier object
  if hasattr(classifier, 'estimator') and hasattr(classifier.estimator, 'pca'):
    ch = classifier.estimator.pca['channels']
  elif hasattr(classifier, 'pca'):
    ch = classifier.pca['channels']
  else:
    raise Exception("Cannot get channel info from classifier")
  ds = ClassifyDataset(args.data, channels=ch, verbose=args.verbose)

  # Predict
  trainer = Classify(classifier, parallel=(not args.disable_parallel), verbose=args.verbose)
  trainer.preprocess(ds, 1)
  pred = trainer.predict(data_ext=data_ext)
  print("Patient, Predicted Label, Dataset Label")
  for r in pred:
    print(f"{r}, {pred[r][0]}, {pred[r][1]}")

def view(args):
  # View augmented dataset
  import pcanet.dataset as dataset
  import numpy as np
  import matplotlib.pyplot as plt
  from matplotlib.widgets import Slider
  if args.data is None:
    raise Exception("No dataset path")
  args.data = os.path.normpath(args.data)
  typename = os.path.basename(args.data).split("_")[-1]
  if typename == "classify":
    ds = dataset.ClassifyDataset(args.data, verbose=args.verbose)
  elif typename == "segment":
    ds = dataset.SegmentationDataset(args.data, verbose=args.verbose)
  else:
    raise Exception(f"Unknown extension: {args.data} ({typename})")

  # Get ids and sizes of aug sets
  pids = [p for p in ds]
  aug_size = [ds.aug_set_size(p) for p in pids]
  total = np.sum(aug_size)
  aug_size = np.cumsum(aug_size)

  # Initial plot
  slices = ds[pids[0]]
  fig, axes = plt.subplots(1,len(ds.channels))
  if isinstance(slices, list):
    fig.suptitle(f"{pids[0][1]} - 000: {pids[0][0].capitalize()}")
  else:
    fig.suptitle(f"{pids[0]} - 000")
  aximg = []
  refidx = 0
  for chn, ch in enumerate(ds.channels):
    if isinstance(slices, list):
      aximg.append(axes[chn].imshow(slices[0][chn,:,:], cmap="gray", vmin=0, interpolation='none'))
    else:
      aximg.append(axes[chn].imshow(slices[0,chn,:,:], cmap="gray", vmin=0, interpolation='none'))
    axes[chn].set_title(ch)
  plt.subplots_adjust(bottom=0.25)
  axslice = plt.axes([0.25,0.1,0.65,0.03])
  slider = Slider(ax=axslice, label='Slice', valmin=1, valmax=total, valinit=0, valstep=1)
  # Update plot on slider change
  def update_ds(num):
    nonlocal refidx, slices, aug_size, ds
    num = int(num)-1
    idx = np.argmax(aug_size > num)
    if idx != refidx:
      slices = ds[pids[idx]]
      refidx = idx
    aid = num if idx == 0 else num - aug_size[idx-1]
    if isinstance(slices, list):
      fig.suptitle(f"{pids[idx][1]} - {aid:03d}: {pids[idx][0].capitalize()}")
    else:
      fig.suptitle(f"{pids[idx]} - {aid:03d}")
    sh = slices[aid][0,:,:].shape
    for chn in range(0,len(ds.channels)):
      if isinstance(slices, list):
        aximg[chn].set_data(slices[aid][chn,:,:])
      else:
        aximg[chn].set_data(slices[aid,chn,:,:])
      aximg[chn].set_extent([0,sh[0],0,sh[1]])
      aximg[chn].set_norm(None)
      aximg[chn].set_clim(0, None)
  slider.on_changed(update_ds)
  fig.set_dpi(Cfg.val['screen_dpi'])
  plt.show(block=True)

if __name__ == '__main__':
  # Only print warnings and errors for tf (set before importing tf)
  if 'TF_CPP_MIN_LOG_LEVEL' not in os.environ:
    os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2' # Do not print info and warning, but errors
  # Find base folder
  if not os.name == 'posix':
    print("**WARNING - PCaNet only runs reliably and is only supported on Linux/POSIX**")
  bin_path = os.path.realpath(__file__)
  if not os.path.isfile(bin_path):
    raise Exception("Cannot find location of mrsnet.py root folder")
  # Initialise cfg
  Cfg.init(bin_path)
  # Headless mode
  if not "DISPLAY" in os.environ:
    from matplotlib import use
    use("Agg")

  main()
